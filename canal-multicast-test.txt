
1. host2 - host3 간 multicast test --> 당근 될꺼고... 이렇게 될꺼라는거 보는거지

iperf는 multicast 성능측정할 수 있음. iperf3는 안됨. 
사용법은 여기에 https://gist.github.com/jayjanssen/5697813

(서버준비)
root@k02:~/workiperf -s -u -B 224.1.1.1 -i 1
------------------------------------------------------------
Server listening on UDP port 5001
Binding to local address 224.1.1.1
Joining multicast group  224.1.1.1
Receiving 1470 byte datagrams
UDP buffer size:  208 KByte (default)
------------------------------------------------------------

(클라이언트에서 쏘기) --> udp이기 때문에 보내기만함. 
root@k03:~/work# iperf -c 224.1.1.1 -u -T 32 -t 3 -i 1
------------------------------------------------------------
Client connecting to 224.1.1.1, UDP port 5001
Sending 1470 byte datagrams
Setting multicast TTL to 32
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
[  3] local 10.1.0.203 port 57341 connected with 224.1.1.1 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   129 KBytes  1.06 Mbits/sec
[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec
[  3] Sent 269 datagrams

(서버에서 받으면)
root@k02:~/workiperf -s -u -B 224.1.1.1 -i 1
------------------------------------------------------------
Server listening on UDP port 5001
Binding to local address 224.1.1.1
Joining multicast group  224.1.1.1
Receiving 1470 byte datagrams
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
[  3] local 224.1.1.1 port 5001 connected with 10.1.0.203 port 57341
[ ID] Interval       Transfer     Bandwidth        Jitter   Lost/Total Datagrams
[  3]  0.0- 1.0 sec   128 KBytes  1.05 Mbits/sec   0.052 ms    0/   89 (0%)
[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec   0.090 ms    0/   89 (0%)
[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec   0.069 ms    0/   89 (0%)
[  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec   0.087 ms    0/  269 (0%)


뭐 대충 이렇게 하면 되겠다... 


2. 같은 namespace 내 pod - pod 간 multicast test  --> 이거 되면 다른 namespace간 되겠지만... 안된다고 하니 해볼수 밖에 

(iperf되는 pod 2개, namespace별로 생성하기)
root@k01:~# kubectl run iperfmc1 --image=mlabbe/iperf -n mc1group --replicas=2
root@k01:~# kubectl run iperfmc2 --image=mlabbe/iperf -n mc2group --replicas=2
root@k01:~# kubectl get po -o wide --all-namespaces
NAMESPACE     NAME                          READY     STATUS    RESTARTS   AGE       IP            NODE
default       iperf3c-c95c4b998-mntw8       1/1       Running   1          47d       10.244.2.24   k03
default       iperf3s-848995697d-tm8lv      1/1       Running   1          47d       10.244.1.40   k02
default       iperfc-7f8bc954d5-h8g8x       1/1       Running   1          47d       10.244.2.22   k03
default       iperfs-5b784d89c-grtgq        1/1       Running   1          47d       10.244.1.38   k02
default       net-test-55c57d8554-k6gpx     1/1       Running   1          48d       10.244.2.23   k03
default       netperfc-76d9c4b567-ckwvd     1/1       Running   1          48d       10.244.1.39   k02
default       netperfs-56bb7779cf-nl54l     1/1       Running   1          48d       10.244.2.25   k03
demos         frontend-2hlnl                1/1       Running   1          48d       10.244.0.7    k01
demos         frontend-z4wjj                1/1       Running   1          48d       10.244.0.8    k01
demos         redis-78zz5                   1/1       Running   1          48d       10.244.0.9    k01
kube-system   canal-89dtx                   3/3       Running   11         56d       10.1.0.201    k01
kube-system   canal-bprl5                   3/3       Running   4          48d       10.1.0.203    k03
kube-system   canal-snrxr                   3/3       Running   14         56d       10.1.0.202    k02
kube-system   etcd-k01                      1/1       Running   3          56d       10.1.0.201    k01
kube-system   kube-apiserver-k01            1/1       Running   3          56d       10.1.0.201    k01
kube-system   kube-controller-manager-k01   1/1       Running   3          56d       10.1.0.201    k01
kube-system   kube-dns-6f4fd4bdf-7b8mv      3/3       Running   6          56d       10.244.0.10   k01
kube-system   kube-proxy-2flp8              1/1       Running   4          56d       10.1.0.202    k02
kube-system   kube-proxy-dbl67              1/1       Running   3          56d       10.1.0.201    k01
kube-system   kube-proxy-jj42n              1/1       Running   1          48d       10.1.0.203    k03
kube-system   kube-scheduler-k01            1/1       Running   3          56d       10.1.0.201    k01
mc1group      iperfmc1-6dd6d6957c-2xqdl     1/1       Running   0          32m       10.244.1.45   k02
mc1group      iperfmc1-6dd6d6957c-mj2gr     1/1       Running   0          32m       10.244.2.30   k03
mc1group      mc1-75f46bfdff-d9q5g          1/1       Running   0          10h       10.244.2.27   k03
mc1group      mc1-75f46bfdff-w4mxk          1/1       Running   0          10h       10.244.1.42   k02
mc2group      iperfmc2-7c5df64b77-kmzh7     1/1       Running   0          32m       10.244.1.46   k02
mc2group      iperfmc2-7c5df64b77-pjv85     1/1       Running   0          32m       10.244.2.31   k03
mc2group      mc2-5bb5fcbcfc-l8rsb          1/1       Running   0          10h       10.244.1.43   k02
mc2group      mc2-5bb5fcbcfc-nwnmb          1/1       Running   0          10h       10.244.2.28   k03


(이중에서 다음 두개 까리 서로 multicast 날려보기로 합니다)
mc1group      iperfmc1-6dd6d6957c-2xqdl     1/1       Running   0          32m       10.244.1.45   k02
mc1group      iperfmc1-6dd6d6957c-mj2gr     1/1       Running   0          32m       10.244.2.30   k03




(서버준비)
root@k01:~# kubectl exec -it iperfmc1-6dd6d6957c-2xqdl -n mc1group -- /bin/sh
/ $ iperf -s -u -B 224.1.1.1 -i 1
------------------------------------------------------------
Server listening on UDP port 5001
Binding to local address 224.1.1.1
Joining multicast group  224.1.1.1
Receiving 1470 byte datagrams
UDP buffer size:  208 KByte (default)
------------------------------------------------------------

--> 패킷이 올때까지 이대로 대기중임. 결론부터... 암것도 안들어옴. 결국 같은 namespace 내로도 multicast 안됨.



(클라이언트준비)
root@k01:~# kubectl exec -it iperfmc1-6dd6d6957c-mj2gr -n mc1group -- /bin/sh
/ $ iperf -c 224.1.1.1 -u -T 32 -t 3 -i 1
------------------------------------------------------------
Client connecting to 224.1.1.1, UDP port 5001
Sending 1470 byte datagrams, IPG target: 11215.21 us (kalman adjust)
Setting multicast TTL to 32
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
[  3] local 10.244.2.30 port 43016 connected with 224.1.1.1 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   131 KBytes  1.07 Mbits/sec
[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec
[  3] Sent 269 datagrams


--> tcp connection이 아니기 때문에 무조건 쏘긴 함. 
    이 상황을 이 pod가 있는 host3에서 tcpdump 잡아보면....
    
 
root@k03:~# tcpdump -i any -nn host 224.1.1.1 -vv
04:54:35.580875 IP (tos 0x0, ttl 32, id 21998, offset 0, flags [DF], proto UDP (17), length 1498)
    10.244.2.30.38810 > 224.1.1.1.5001: [bad udp cksum 0xf3eb -> 0xeb87!] UDP, length 1470
04:54:35.592583 IP (tos 0x0, ttl 32, id 22001, offset 0, flags [DF], proto UDP (17), length 1498)
    10.244.2.30.38810 > 224.1.1.1.5001: [bad udp cksum 0xf3eb -> 0xbdcd!] UDP, length 1470
04:54:35.603482 IP (tos 0x0, ttl 32, id 22002, offset 0, flags [DF], proto UDP (17), length 1498)
    10.244.2.30.38810 > 224.1.1.1.5001: [bad udp cksum 0xf3eb -> 0x9339!] UDP, length 1470
    .... 계속 잡힘. 
    
    
--> 하지만 이건 전체 인터페이스 대상으로 잡은거라 잡히는 것임. 
    옆 호스트로는 한 패킷도 안옴. 즉, canal-flannel-ens33으로 연결안해줌. 
    
    
3. 혹시나 두 pod가 같은 node에 있다면? 가능할까?? 해서...

root@k01:~# kubectl scale deployment/iperfmc1 --replicas=4 -n mc1group
deployment "iperfmc1" scaled
root@k01:~# kubectl get po -o wide -n mc1group
NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE
iperfmc1-6dd6d6957c-2xqdl   1/1       Running   0          40m       10.244.1.45   k02
iperfmc1-6dd6d6957c-9tt2r   1/1       Running   0          4s        10.244.1.47   k02
iperfmc1-6dd6d6957c-mj2gr   1/1       Running   0          40m       10.244.2.30   k03
iperfmc1-6dd6d6957c-tljxp   1/1       Running   0          1m        10.244.0.11   k01
mc1-75f46bfdff-d9q5g        1/1       Running   0          11h       10.244.2.27   k03
mc1-75f46bfdff-w4mxk        1/1       Running   0          11h       10.244.1.42   k02

이제 위에 두개에 iperf -s 해보자구
iperfmc1-6dd6d6957c-2xqdl   1/1       Running   0          40m       10.244.1.45   k02
iperfmc1-6dd6d6957c-9tt2r   1/1       Running   0          4s        10.244.1.47   k02



root@k01:~# kubectl exec -it iperfmc1-6dd6d6957c-2xqdl -n mc1group -- /bin/sh
/ $ iperf -s -u -B 224.1.1.1 -i 1
------------------------------------------------------------
Server listening on UDP port 5001
Binding to local address 224.1.1.1
Joining multicast group  224.1.1.1
Receiving 1470 byte datagrams
UDP buffer size:  208 KByte (default)
------------------------------------------------------------


root@k01:~# kubectl exec -it iperfmc1-6dd6d6957c-9tt2r  -n mc1group -- /bin/sh
/ $ iperf -s -u -B 224.1.1.1 -i 1
------------------------------------------------------------
Server listening on UDP port 5001
Binding to local address 224.1.1.1
Joining multicast group  224.1.1.1
Receiving 1470 byte datagrams
UDP buffer size:  208 KByte (default)
------------------------------------------------------------


(그리고 node 3에서 쏘기)  --> 앞에 두 pod 모두 반응없음. 
root@k01:~# kubectl exec -it iperfmc1-6dd6d6957c-mj2gr -n mc1group -- /bin/sh
/ $ iperf -c 224.1.1.1 -u -T 32 -t 3 -i 1
------------------------------------------------------------
Client connecting to 224.1.1.1, UDP port 5001
Sending 1470 byte datagrams, IPG target: 11215.21 us (kalman adjust)
Setting multicast TTL to 32
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
[  3] local 10.244.2.30 port 33719 connected with 224.1.1.1 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   131 KBytes  1.07 Mbits/sec
[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec
[  3] Sent 269 datagrams





(그렇다면, node2에 두 pod에서 서로 server client 놀이하면? 가능할까? )  --> 안됨~! 같은 node 내에 있어도 multicast는 안됨. 

root@k01:~# kubectl exec -it iperfmc1-6dd6d6957c-9tt2r  -n mc1group -- /bin/sh
/ $ iperf -s -u -B 224.1.1.1 -i 1
------------------------------------------------------------
Server listening on UDP port 5001
Binding to local address 224.1.1.1
Joining multicast group  224.1.1.1
Receiving 1470 byte datagrams
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
/ $ iperf -c 224.1.1.1 -u -T 32 -t 3 -i 1
------------------------------------------------------------
Client connecting to 224.1.1.1, UDP port 5001
Sending 1470 byte datagrams, IPG target: 11215.21 us (kalman adjust)
Setting multicast TTL to 32
UDP buffer size:  208 KByte (default)
------------------------------------------------------------
[  3] local 10.244.1.47 port 51206 connected with 224.1.1.1 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   131 KBytes  1.07 Mbits/sec
[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec
[  3] Sent 269 datagrams


(마지막으로 같은 namespace 내에 다른 pod에서 multicast traffic이 잡히는지 ?  --> multicast, broadcast 안잡힘. 
root@k01:~# kubectl run nettool1 --image=jonlangemak/net_tools -n mc1group --replicas=1
deployment "nettool1" created
root@k01:~# kubectl get po -o wide -n mc1group
NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE
iperfmc1-6dd6d6957c-2xqdl   1/1       Running   0          48m       10.244.1.45   k02
iperfmc1-6dd6d6957c-9tt2r   1/1       Running   0          7m        10.244.1.47   k02
iperfmc1-6dd6d6957c-mj2gr   1/1       Running   0          48m       10.244.2.30   k03
iperfmc1-6dd6d6957c-tljxp   1/1       Running   0          9m        10.244.0.11   k01
mc1-75f46bfdff-d9q5g        1/1       Running   0          11h       10.244.2.27   k03
mc1-75f46bfdff-w4mxk        1/1       Running   0          11h       10.244.1.42   k02
nettool1-7bf64c4b8b-l8kr5   1/1       Running   0          20s       10.244.2.32   k03


root@k01:~# kubectl exec -it nettool1-7bf64c4b8b-l8kr5 -n mc1group -- /bin/bash
root@nettool1-7bf64c4b8b-l8kr5:/# tcpdump -i any -nn host 224.1.1.1 -vv
tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes
^C
0 packets captured
0 packets received by filter
0 packets dropped by kernel
root@nettool1-7bf64c4b8b-l8kr5:/# tcpdump -i any  -vv
tcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes
^C
0 packets captured
0 packets received by filter
0 packets dropped by kernel
root@nettool1-7bf64c4b8b-l8kr5:/# tcpdump -n broadcast or multicast
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
^C
0 packets captured
0 packets received by filter
0 packets dropped by kernel
root@nettool1-7bf64c4b8b-l8kr5:/#

